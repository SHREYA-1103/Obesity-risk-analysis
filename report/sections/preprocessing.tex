\chapter{Data Preprocessing & Feature Engineering}\label{ch:feature-engineering}


This is one of the most crucial stages in the machine learning pipeline, as it directly influences how effectively models can learn patterns and relationships from the data.
This process involves transforming raw features into formats that enhance model interpretability, reduce bias, and improve predictive performance.
In this project, several transformations were applied to handle skewed numerical variables, encode categorical variables, and ensure that both linear and tree-based models received data in the most suitable form for training.

%----------------------------------------------------------------


\section{Transformation of Numerical Features}\label{sec:transformation-of-numerical-features}

Upon analyzing the numerical variables during the exploratory phase, it was observed that several features, particularly Age, NCP (Number of Main Meals), FCVC (Frequency of Vegetable Consumption), and CH2O (Daily Water Intake), displayed right-skewed distributions.
Skewed data can adversely affect models, especially linear algorithms such as Logistic Regression, which assume a near-normal distribution of features.
Therefore, to improve normality and stabilize variance, various mathematical transformations were tested on these features, including logarithmic, square root, and Box‚ÄìCox transformations.

For the Age attribute, all three transformation methods were applied, and their effects on the distribution were analyzed using histograms and Q‚ÄìQ plots.
The Box‚ÄìCox transformation produced the most symmetric and approximately normal distribution, significantly reducing skewness compared to log and square root transformations.
Therefore, the Box‚ÄìCox transformation was selected for Age as the optimal approach.
In contrast, for Weight, none of the applied transformations (log, square root, or Box‚Äì Cox) resulted in substantial improvement in distribution or model performance.
Since the weight feature already had a relatively stable and less skewed distribution, it was retained in its original form to preserve interpretability and avoid unnecessary distortion of values.
Mathematically, the Box‚ÄìCox transformation can be defined as:

\begin{equation}
    \centering
    x^\prime=\ \left\{
                   \begin{matrix}
                       \frac{x^\lambda}{\lambda}&if\ \lambda\neq0\\
                       \ln\funcapply(x)&if\ \lambda=0\\
                   \end{matrix}\right.
    \label{eq:eq_box_cox}
\end{equation}

where $x$ represents the feature value and ùúÜis a parameter determined through maximum likelihood estimation that minimizes skewness.
This transformation is particularly advantageous because it dynamically adapts to the data‚Äôs distribution rather than applying a fixed transformation across all variables.

%----------------------------------------------------------------


\section{Handling of Categorical Variables}\label{sec:handling-of-categorical-variables}

The dataset contained several categorical attributes such as Gender, family history of CVD, smoking habits, transportation means, and physical activity level, among others.
These features required encoding before being passed to machine learning models, as most algorithms cannot operate directly on non-numeric data.

Given the differences in how various algorithms interpret feature relationships, two distinct encoding strategies were employed-one for linear models and another for tree-based models:

\begin{description}
    \item[\textbf{For Linear Models:}]
    Linear models (including Regression-based, Distance-based and PRobability-based models) rely on distance-based calculations and assume linearity between predictors and target variables.
    To prevent the model from incorrectly inferring ordinal relationships among categories, One-Hot Encoding (OHE) was applied.
    In this encoding scheme, each category of a variable is converted into a separate binary feature, ensuring that no ordinal bias is introduced.
    Although OHE increases dimensionality, it allows the model to learn independent effects of each category accurately.
    After encoding, numerical features were standardized using StandardScaler to normalize feature magnitudes.

    \item[\textbf{For Tree-Based Models:}]
    Tree-based models are inherently insensitive to monotonic transformations and do not assume linear relationships between input features.
    Therefore, Label Encoding was employed for categorical variables in these models.
    Label Encoding assigns a unique integer to each category, which the model interprets as a discrete split criterion rather than a numerical ranking.
    This method is computationally efficient, reduces memory overhead, and avoids the curse of dimensionality introduced by OHE. This dual-encoding strategy ensured that each model type received appropriately formatted input features-linear models benefitted from interpretability and numerical uniformity through OHE, while tree-based models leveraged label-encoded data for computational efficiency and faster convergence
\end{description}

%----------------------------------------------------------------


\section{Feature Scaling}\label{sec:feature-scaling}

Feature scaling was another key aspect of the preprocessing pipeline.
Since numerical features were measured on different scales (e.g., Age in years, Height in meters, Weight in kilograms), unscaled data could bias models that depend on distance metrics or gradient optimization.
Hence, StandardScaler from scikit-learn was applied to all numerical features.
StandardScaler transforms each feature to have a mean of zero and a standard deviation of one, according to the formula:

\begin{equation}
    \centering
    x=\frac{x-\mu}{\sigma}
    \label{eq:eq_standard_Scaling}
\end{equation}

where $\mu$ and $\sigma$ represent the feature‚Äôs mean and standard deviation, respectively.
This step was especially crucial for algorithms such as Logistic Regression, K-Nearest Neighbors (KNN), and Na√Øve Bayes, which are sensitive to feature magnitude differences.
For tree-based models like Random Forest and XGBoost, scaling was not mandatory, as these models are invariant to monotonic transformations, but it was still maintained for consistency and comparative evaluation.

%----------------------------------------------------------------


\section{Derived and Interaction Features}\label{sec:derived-and-interaction-features}

In addition to transformation and encoding, several experiments were conducted to test the effect of derived features on model performance.
Derived features are new attributes constructed by combining or transforming existing ones to better represent underlying patterns.
For instance, features such as BMI-related interactions and agediet combinations were explored.
However, it was observed that introducing too many derived features led to marginal performance improvement but increased model complexity and risk of overfitting.
Therefore, only the most meaningful derived features-those contributing to consistent accuracy gains-were retained in the final dataset.

Comparative testing between datasets with and without derived features revealed that while certain linear models benefited slightly from derived interactions, ensemble models like XGBoost and LightGBM were already capable of learning non-linear interactions internally through feature splitting, making external feature engineering less impactful for them.

%----------------------------------------------------------------


\section{Preparation of Model-Specific Datasets}\label{sec:preparation-of-model-specific-datasets}

To maintain an efficient and organized workflow, two separate datasets were prepared post-feature engineering:

\begin{enumerate}
    \item \textbf{Dataset A (for Logistic Regression and other linear models):}
    \begin{itemize}
        \item Applied Box‚ÄìCox transformation to Age, NCP, FCVC, and CH2O.
        \item One-Hot Encoded all categorical features.
        \item Applied StandardScaler to all numerical features. \\ This dataset ensured numerical stability and feature independence for models relying on linear assumptions.
    \end{itemize}

    \item \textbf{Dataset B (for tree-based models such as XGBoost, LightGBM, Random Forest, and Decision Tree):}
    \begin{itemize}
        \item Applied Box‚ÄìCox transformation to the same set of skewed numerical features.
        \item Label Encoded categorical variables.
        \item Retained numerical features in their scaled form for consistency. \\ This dataset was designed to preserve the hierarchical and non-linear interpretive capability of tree-based algorithms while reducing unnecessary dimensionality.
    \end{itemize}
\end{enumerate}

%----------------------------------------------------------------


\section{Summary of Feature Engineering Outcomes}\label{sec:summary-of-feature-engineering-outcomes}

The feature engineering process significantly improved the quality and structure of the dataset, enabling better model learning and convergence.
Box‚ÄìCox transformation successfully normalized skewed distributions, enhancing the performance of models sensitive to feature variance.
The dual-encoding approach optimized categorical variable handling across different algorithm types, ensuring both interpretability and computational efficiency.
Additionally, careful evaluation of derived features helped maintain a balance between model complexity and generalization ability.

Through these systematic transformations, the dataset was effectively prepared for model training and validation, laying the foundation for robust and high-performing predictive models.
The improvements achieved during this stage were reflected in the final results, where optimized feature representation contributed to a remarkable increase in accuracy, culminating in the XGBoost model achieving 99\% accuracy on test data.
