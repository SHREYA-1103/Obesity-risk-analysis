\chapter{Methodology}\label{ch:methodology}


In general, a standard methodology for machine learning projects follows a linear process - loading the dataset, visualizing it, performing data cleaning and preprocessing, training models, and finally generating the submission outputs.

However, building upon insights from prior works on similar data-pipelining tasks, this project adopts a more mature and iterative framework rather than a purely linear one.
For instance, data visualization is no longer treated as a standalone phase; instead, it is an ongoing process integrated throughout the workflow - from data loading and preprocessing to model training and evaluation.
This continuous visualization strategy enables early detection of anomalies and fosters a deeper understanding of model–data interactions at every stage.

Furthermore, the need for abstraction and modularity becomes evident when conducting comparative studies involving multiple models and experiments.
Several experiments share similar configurations or preprocessing routines, and to handle this efficiently, the project employs helper methods and modular utilities that encapsulate repeated processes.
This abstraction ensures code clarity, minimizes redundancy, and simplifies systematic experimentation.

In this report, a custom pipelining infrastructure is presented to enhance transparency and control over each step of the process.
Unlike the conventional scikit learn pipeline, this system provides richer insights into the effectiveness of individual preprocessing strategies, offering fine-grained visualization of their impact on model performance.

Lastly, leveraging lessons from earlier projects, the implementation maintains rigorous control over reproducibility aspects such as random seed management, model versioning, and grid search configurations, ensuring consistent and reliable experimentation across runs.

\importPlotFigure{figures/plot_Model Comparison.png}{Model Comparison}{model_time_comparison}


\section{Dataset Description}\label{sec:dataset-description}
The data loading phase follows a straightforward and minimal approach.
The primary dataset is loaded into {ds_source}, and the submission dataset into {ds_test}.
An additional preprocessing step involves renaming the column family_history_with_overweight to FHWO, aligning it with the naming convention of other acronym-based attributes.
This minor modification simplifies table formatting, improves readability, and ensures consistency across visualizations and statistical summaries.
The primary dataset is loaded into \texttt{\{ds\_source\}} and the submission dataset into \texttt{\{ds\_test\}}.


\section{Exploratory Data Analysis (EDA)}\label{sec:exploratory-data-analysis}
The Exploratory Data Analysis (EDA) phase focuses on identifying missing values, outliers, and other statistical anomalies within the dataset.
Although this section might appear procedural, its purpose extends beyond aesthetic visualization - it aims to extract meaningful insights that guide data preprocessing and modeling.

A variety of plots and statistical summaries are generated to ensure a comprehensive understanding of data distributions, correlations, and feature relationships.
Care is taken to emphasize interpretability - each visualization contributes directly to understanding the dataset’s behavior and influences the decisions made during feature engineering and preprocessing.


\section{Feature Engineering & Preprocessing}\label{sec:data-preprocessing}
The feature engineering and preprocessing stage forms the backbone of the entire modeling workflow, translating raw data into structured, meaningful representations suitable for machine learning algorithms.
All transformations applied at this stage are informed by insights drawn from the exploratory data analysis (EDA). Rather than mutating the original data source, the transformations are encapsulated within modular units known as Pipeline Operations (POPs), which maintain full traceability and reversibility of each preprocessing step.

\subsection{Transformation Strategy}\label{subsec:transformation-strategy}
The primary objective of this phase is to enhance the interpretability and predictive strength of the dataset while ensuring model compatibility across diverse algorithmic families.
Both numerical and categorical attributes undergo distinct transformation and encoding schemes, tailored to the type of model being trained (linear or treebased).

\textbf{Numerical Transformations}

During distribution analysis, several continuous variables such as Age, NCP, CH₂O, and FCVC displayed moderate skewness and non-Gaussian patterns.
To address this, multiple transformation techniques - logarithmic, square root, and Box–Cox transformations - were systematically tested.

For the Age attribute, the Box–Cox transformation yielded the most statistically balanced distribution, effectively reducing skewness and improving normality, whereas the log and square root transformations provided minimal improvement.

For Weight, none of the tested transformations led to a significant improvement in distribution symmetry; thus, the original scale was retained.

These transformations were validated through post-transformation histograms, Q–Q plots, and normality tests to ensure stability before being incorporated into the final pipeline.

\textbf{Categorical Transformations}

The dataset contained multiple categorical features, each influencing the model differently depending on the algorithm used.
To accommodate this, dual-encoding strategies were applied:

For linear models such as Logistic Regression, categorical variables were One-Hot Encoded (OHE) to maintain orthogonality and prevent unintended ordinal relationships.
Additionally, a specialized soft one-hot encoding (soft OHE) technique was applied to features like CH₂O, which exhibited discrete peaks within a continuous range.

For tree-based models such as Decision Tree, Random Forest, XGBoost, and LightGBM, categorical features were Label Encoded, preserving computational efficiency and minimizing feature dimensionality.

This dual-encoding design ensured that each model family received the most suitable representation of categorical information, maximizing predictive performance without introducing redundancy.

\textbf{Feature Derivation}

Beyond transformation, several derived features were engineered to capture latent relationships among existing attributes.
These were designed based on domain intuition about obesity and cardiovascular risk factors.
Derived features included:

BMI – Calculated from height and weight to provide a normalized indicator of body composition.

Water\_Intake\_per\_Meal – Derived from CH₂O and NCP to approximate hydration behavior per meal.

Activity\_to\_Tech\_Ratio – Ratio of physical activity levels to technology usage time, representing lifestyle balance.

Healthy\_Lifestyle\_Score – Weighted aggregate of positive lifestyle indicators such as high FCVC, adequate CH₂O, and regular physical activity.

Has\_FamilyRisk\_and\_FAVC – Interaction feature capturing the combined impact of genetic predisposition and high-calorie food consumption.

Calorie\_Monitoring\_Interaction – Interaction between food frequency (FAF) and calorie monitoring (CAEC), approximating dietary awareness.

These engineered features significantly improved the feature space’s expressiveness and allowed models to capture nuanced behavioral and physiological interactions relevant to obesity and cardiovascular risk.

\textbf{Scaling and Standardization}

To ensure uniform feature contribution, StandardScaler was applied to all numerical features.
This transformation standardized variables to zero mean and unit variance, a critical step for models sensitive to feature magnitudes such as Logistic Regression and K-Nearest Neighbors.
For tree-based models, which are scale-invariant, raw or label-encoded data were retained to preserve interpretability.

\textbf{Oversampling and Data Balance}

Given the class imbalance observed in obesity–CVD categories, Synthetic Minority Oversampling Technique (SMOTE) was used to generate synthetic minority samples\. The sample size increased from approximately 15,000 to 20,000.
This significantly enhanced the model’s ability to recognize minority patterns, resulting in up to 3\% accuracy improvements in XGBoost, the best-performing model.

All preprocessing operations were implemented as POPs, enabling modular combination, visualization, and reproducibility across experiments.


\section{Setups and Helpers}\label{sec:setups-and-helpers}
Although this section does not directly appear in the final report output, it plays a foundational role within the notebook implementation.
Executed immediately after the import statements, it handles several preliminary configurations essential for smooth experimentation.
These include:

Initialization of the notification and logging system

PyPlot and visualization styling adjustments

Random seed management for reproducibility

Notebook display and formatting controls

Model training, saving, and submission automation

Definition and registration of POP operations

\subsection{Pipeline Operations (POPs)}\label{subsec:pipeline-operations-(pops)}
Each POP is implemented as a modular function that takes a data object (with optional configuration parameters) and returns a tuple containing the transformation configuration and the processed dataset.
This design encourages composability and experimentation across different transformation strategies.

The implemented POPs include:

\begin{itemize}
    \item pop\_drop\_column
    \item pop\_log\_transform
    \item pop\_root\_transform
    \item pop\_box\_cox\_transform
    \item pop\_one\_hot\_encode
    \item pop\_soft\_ohe
    \item pop\_binarize
    \item pop\_standardize
    \item pop\_minmax
    \item pop\_ordinal\_encode
    \item pop\_round
    \item pop\_derived\_features
\end{itemize}

This modular approach enables quick mixing and matching of transformations, facilitating the discovery of the most effective preprocessing combinations through systematic experimentation.

\subsection{Helper Utilities}\label{subsec:helper-utilities}
The POP infrastructure is supported by several helper functions designed to streamline experimentation and simplify configuration management.
These include:

\textbf{compose\_pop} – Combines multiple POPs into a single unified operation.

\textbf{prepare\_pop} – Finalizes a POP by fixing its configuration parameters.

\textbf{select\_pipeline\_variation} – Chooses the most promising pipeline configuration among various permutations.

\textbf{apply\_pipeline} – Applies a series of pipeline variations to the dataset and evaluates their effects.

Together, these functions ensure the infrastructure remains modular, scalable, and adaptive to multiple model setups.

\subsection{Specialized POPs}\label{subsec:specialized-pops}
Among the implemented transformations, two specialized POPs deserve particular mention:

\textbf{pop\_soft\_ohe} – Performs a “soft” one-hot encoding for features exhibiting discrete peaks within an otherwise continuous distribution.
For example, in the CH₂O feature, distinct spikes occur at values 1, 2, and 3.
This transformation isolates these regions, creating dedicated features that capture each peak more effectively.

\textbf{pop\_derived\_features} – Generates higher-level derived features from existing columns.
These engineered features capture latent relationships and health-related behavioral indicators such as:

BMI

Water\_Intake\_per\_Meal

Activity\_to\_Tech\_Ratio

Healthy\_Lifestyle\_Score

Has\_FamilyRisk\_and\_FAVC

Calorie\_Monitoring\_Interaction

Together, these components establish a robust experimental foundation, ensuring that every transformation and pipeline decision can be systematically analyzed, visualized, and reproduced across experiments.


\section{Model Training and Evaluation}\label{sec:model-training-and-evaluation}
The final phase of the methodology involves model construction, hyperparameter optimization, and performance evaluation.
The training framework was designed to maintain consistency across experiments while enabling model-specific customization.

\subsection{Data Splitting}\label{subsec:data-splitting}
The preprocessed dataset was split into training (70\%), and validation (30\%) subsets using Stratified Sampling to maintain proportional class representation across all splits.
Separate data configurations were maintained for:

Linear models (Logistic Regression, KNN, Naive Bayes): using OHE-transformed datasets.

Tree-based models (Decision Tree, Random Forest, AdaBoost, XGBoost, LightGBM): using label-encoded datasets.

\importPlotFigure{figures/plot_Feature Distribution (Discrete).png}{Discrete Features Distribution}{feature_distribution_discrete}

\subsection{Model Selection and Training}\label{subsec:model-selection-and-training}
A diverse suite of models was implemented to evaluate various learning paradigms:

Logistic Regression – Served as the baseline linear classifier, evaluated using regularization (L1, L2) and solver variations.

Decision Tree Classifier – Provided interpretability and served as the foundation for ensemble models.

Random Forest Classifier – Leveraged ensemble bagging to reduce variance and improve robustness.

AdaBoost Classifier – Introduced boosting-based iterative learning to handle hard-toclassify instances.

K-Nearest Neighbors (KNN) – Captured local instance-based decision boundaries.

Naive Bayes – Provided a probabilistic baseline assuming feature independence.

XGBoost Classifier – Gradient boosting model optimized for speed and accuracy, offering strong generalization capability.

LightGBM Classifier – Gradient boosting framework optimized for memory and computational efficiency.
\\ \\
All models were trained on the training set and evaluated on the validation and test sets.

\subsection{Hyperparameter Optimization}\label{subsec:hyperparameter-optimization}
Different optimization approaches were used depending on model complexity:

For Logistic Regression, Decision Tree, Random Forest, AdaBoost, KNN, and Naive Bayes, Grid Search Cross-Validation was used to explore parameter combinations systematically.

For XGBoost and LightGBM, Optuna was employed for Bayesian hyperparameter optimization using Stratified K-Fold Cross-Validation (k=5).
Optuna dynamically adjusted hyperparameters based on prior results, efficiently converging to optimal configurations.

This hybrid tuning strategy balanced exploration and computational efficiency while ensuring consistency across model evaluations.

\subsection{Evaluation Metrics}\label{subsec:evaluation-metrics}
Model performance was primarily evaluated using accuracy, given the balanced dataset post-SMOTE.
Additional metrics such as precision, recall, F1-score, and confusion matrix were used for detailed class-level analysis.

Each model’s predictions were also validated on a separate Kaggle submission dataset, ensuring the robustness of the trained models on unseen data.

The best accuracy of 93.2\% was achieved using XGBoost, followed closely by LightGBM, both of which benefited from the rich feature set, proper handling of categorical encodings, and SMOTE-based class balance.
